{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "environment": {
      "name": "common-cpu.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# this is the Python code for a forecast solution I built at BDT\n# BDT sends several kinds of outreach letters to various outreach groups, inviting them to call the BDT call center \n# for help with submitting benefit applications.\n\n# This code generates a time series forecast for each outreach line (a combination of state / letter # / outreach group / attempt / target benefit)\n# the goal is to predict the number of calls/application submissions that the call center will see on a given week \n# for computational efficiency, some forecasting functions are handled in stored procedures in BigQuery (not included in this notebook)\n\n# this code has been prepared for converting the notebook into a cloud function, so all the helper functions come first, and the main functions\n# (fcst_main, seq_validation) are at the bottom\n\nimport pandas as pd\nfrom google.cloud import bigquery\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nfrom datetime import datetime, time, timedelta\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom statsmodels.tsa.exponential_smoothing.ets import ETSModel\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom dateutil.relativedelta import relativedelta, FR\n\nbigquery_client=bigquery.Client()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# function initialize_forecast\n# Initialize the forecast with the specified parameters.\n# Args:\n#   live (int, optional): Flag to indicate if the forecast will be live. Default is 0 (not live).\n#   visible (int, optional): Flag to indicate if the (historic) forecast should be visible in the dashboard. Default is 0 (not visible).\n#   ftype (str, optional): Type of forecast: is this a response forecast (R) or a submission forecast (S). Default is 'R' (e.g., 'R' for regular).\n#   refdate (str, optional): Reference date for the forecast: i.e. let's pretend today is this date and we don't know anything that has happened since. \n#     Used for generating historical forecasts for validation. Default is an empty string, means today's date will be used.\n#   params (dict, optional): parameters for the forecast. Include only those parameters where you want to overrride the default parameter value.\n#                Default is an empty dictionary.\n#\n# Returns:\n#   fcst_id (uuid): the identifier for the forecast\n#   refdate (date): reference date adjusted to previous Friday\n#   params (list): values for all parameters (default or overridden)\n\ndef initialize_forecast(live = 0, visible = 0, ftype = 'R', refdate = '',params = {}):\n    if (refdate != ''):\n        rdatecode = \"DATE_TRUNC('\"+refdate +\"', WEEK(FRIDAY))\"\n    else:\n        rdatecode = \"DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 4 DAY), WEEK(FRIDAY))\"\n\n    fcstframe = bigquery_client.query(\"SELECT GENERATE_UUID() as id, \" + rdatecode + \" as rdate\").to_dataframe()\n    fcst_id = fcstframe[\"id\"][0]\n    rdate = fcstframe[\"rdate\"][0]\n    \n    if (live == 1):\n        fcstupdate = bigquery_client.query(\"\"\"UPDATE lab_forecast.fcst SET live = 0 WHERE live = 1\"\"\")    \n        fcstupdate.result()\n            \n    fcstinsert = bigquery_client.query(\"\"\"INSERT INTO lab_forecast.fcst (generated_at, reference_date, fcst_type, fcst_uuid, live, visible)\n                                         VALUES (CURRENT_TIMESTAMP(), \"\"\" + rdatecode + \"\"\",'\"\"\" + ftype + \"\"\"','\"\"\" + fcst_id+\"\"\"',\n                                                 \"\"\"+str(live)+\"\"\", \"\"\"+str(visible)+\"\"\")\"\"\")    \n    fcstinsert.result()\n    \n    myparams = bigquery_client.query(\"\"\"SELECT parameter_uuid, parameter_name, default_value as par_value, min_value, max_value \n                                     FROM lab_forecast.fcst_parameter_list\"\"\").to_dataframe()\n    for key,value in params.items():\n        if (myparams.loc[myparams[\"parameter_name\"]==key,\"min_value\"].values[0] > value):\n            value = myparams.loc[myparams[\"parameter_name\"]==key,\"min_value\"].values[0]\n            print(key + \" value provided is too low. Resetting to \" + str(value))\n        if (myparams.loc[myparams[\"parameter_name\"]==key,\"max_value\"].values[0] < value):\n            value = myparams.loc[myparams[\"parameter_name\"]==key,\"max_value\"].values[0]\n            print(key + \" value provided is too high. Resetting to \" + str(value))    \n        myparams.loc[myparams[\"parameter_name\"]==key,\"par_value\"] = value\n    \n    myparams = myparams.set_index(\"parameter_name\")\n    myparams = myparams[[\"parameter_uuid\",\"par_value\"]]\n    myparams[\"fcst_uuid\"] = fcst_id    \n        \n    ptable = bigquery_client.dataset(\"lab_forecast\").table(\"fcst_parameters\")\n    rows_to_insert = myparams.to_dict(orient='records')       \n    errors = bigquery_client.insert_rows(bigquery_client.get_table(ptable), rows_to_insert) \n    \n    if (errors):\n        print(errors)\n    \n    return fcst_id, rdate, myparams.to_dict().get(\"par_value\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# function create_lines: find a list of forecast lines (state/letter/outreach group/attempt/benefit) that could be contributing to \n# the call/application volume during the forecast period [refdate, refdate + forecast_horizon]\n# because response often takes time, this could include any letter sent during the previous response_weeks weeks as well as letters\n# that are planned for the future\n\n# the function calls a stored procedure that precomputes certain values based on how much individual history there is for each line\n\n# lines that have been in operation less than own_history_weeks_threshold_for_response weeks will be forecasted using a combination of own data and \n# and pooled data from similar lines\n\n# similarly, lines where fewer than own_history_letters_threshold_for_delay letters have received a response will have the delay probability\n# distribution calculated based on a combination of own and pooled data\n\n# Args:\n#  fcst_id (uuid): unique identifier for the forecast\n#  params (list): list of parameter values\n# Returns:\n#  lines_needed (dataframe): list of lines and their properties\n\ndef create_lines(fcst_id, params):\n    create_query = bigquery_client.query(\"\"\"CALL lab_forecast.create_lines('\"\"\" + fcst_id + \"\"\"',\"\"\" + str(round(params[\"response_weeks\"])) + \"\"\",\n                                    \"\"\" + str(round(params[\"forecast_horizon\"])) + \"\"\",\n                                    \"\"\" + str(round(params[\"offset_weeks\"])) + \"\"\",\"\"\" + str(round(params[\"cutoff_weeks\"])) + \"\"\",\n                                    \"\"\" + str(round(params[\"own_history_weeks_threshold_for_response\"])) + \"\"\",\n                                    \"\"\" + str(round(params[\"own_history_letters_threshold_for_delay\"])) + \"\"\")\"\"\")\n    create_query.result()\n    \n    lines_needed = bigquery_client.query(\"\"\"SELECT a.*, b.state, LEAST(b.mail_attempt,2) as attempt, b.target_benefit as benefit\n                                            FROM lab_forecast.fcst_fcst_lines a,\n                                                 lab_forecast.fcst_lines b\n                                            WHERE a.line_uuid = b.line_uuid and fcst_uuid = '\"\"\" + fcst_id + \"\"\"'\n                                            ORDER BY a.line_uuid\"\"\").to_dataframe() \n    return lines_needed",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# function plot_response_rates: generates a time series plot of adjusted historical response rates\n# Args:\n#  fcst_id (uuid): unique identifier for the forecast\n\ndef plot_response_rates(fcst_id):\n    hrates = bigquery_client.query(\"\"\"SELECT * FROM lab_forecast.fcst_hist_rates WHERE rate is not null and fcst_uuid = '\"\"\" + fcst_id + \"\"\"' \n                               order by line_uuid, rate_date\"\"\").to_dataframe()\n    itable = pd.pivot_table(hrates, values='rate', index=['rate_date'],\n                        columns=['line_uuid'], aggfunc=np.sum)\n    itable.plot()",
      "metadata": {},
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": "# function get_delay_query: builds a SQL query for retrieving the response delay historical distribution\n# Args:\n#  fcst_id (uuid): unique identifier for the forecast\n#  response_weeks (int): threshold for the number of response weeks to include.\n\ndef get_delay_query(fcst_id, response_weeks):\n    query = \"\"\"SELECT \n    TO_HEX(MD5(concat(m.state, '##', m.letter_number, '##', m.mail_outreach_group, '##',m.mail_attempt, '##',m.mail_target_benefit))) as line_id,\n    m.state, LEAST(m.mail_attempt,2) as attempt, m.mail_target_benefit as benefit,\n    cast(DATE_ADD(DATE_TRUNC(m.mail_scheduled_at, WEEK), INTERVAL 5 DAY) as date) as mailwk,  \n    m.mailing_uuid, cast(date_sub(date_TRUNC(pc.created_at, WEEK),INTERVAL 2 DAY) as date) as callwk     \n        FROM \n        lab_forecast.fcst f,\n        `bdt-data-sci-prod.model_benefit_applications.f_mailing` m,\n        `bdt-data-sci-prod.model_benefit_applications.f_phone_call` pc\n        WHERE\n        f.fcst_uuid = '\"\"\"+fcst_id+\"\"\"'\n        and pc.last_touch_mailing_uuid = m.mailing_uuid\n        and cast(pc.created_at as date) <= f.reference_date\n        and cast(DATE_ADD(DATE_TRUNC(m.mail_scheduled_at, WEEK), INTERVAL 5 DAY) as date) >= \n                       DATE_SUB(f.reference_date, INTERVAL  \"\"\"+str(response_weeks)+\"\"\" WEEK)\"\"\"\n\n    return query    ",
      "metadata": {},
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": "# function get_delay_frequencies: computes the probability distribution of response delay\n# using a combination of individual and pooled data\n# Args:\n#  fcst_id (uuid): unique identifier for the forecast\n#  params (list): forecast parameters\n#  lines_needed (dataframe): list of lines and their parameters\n#  compute_agg_rates (boolean): whether the aggregated rates should be recomputed\n\ndef get_delay_frequencies(fcst_id, params, lines_needed, compute_agg_rates = True):\n    cutoff_weeks = round(params[\"cutoff_weeks\"])\n    response_weeks = round(params[\"response_weeks\"])\n    own_history_letters = round(params[\"own_history_letters_threshold_for_delay\"])\n    delay_query = get_delay_query(fcst_id, response_weeks)\n    delay_df = bigquery_client.query(delay_query).to_dataframe()\n   \n    delay_df['period'] = np.maximum(np.minimum((delay_df['callwk']-delay_df['mailwk']).dt.days/7,cutoff_weeks+1),0)\n          \n    result_all = pd.DataFrame()\n    if (compute_agg_rates):         \n        result_all = get_delay_frequency_table(delay_df, cutoff_weeks)      \n       \n        agg_lines = delay_df.groupby([\"state\",\"benefit\",\"attempt\"], as_index=False)[\"mailing_uuid\"].count()\n        for s in range(len(agg_lines.index)):    \n            delay_df_sub = delay_df.loc[(delay_df[\"state\"]==agg_lines[\"state\"][s]) & (delay_df[\"benefit\"]==agg_lines[\"benefit\"][s]) & \n                      (delay_df[\"attempt\"]==agg_lines[\"attempt\"][s]),]\n            result_sub = get_delay_frequency_table(delay_df_sub, cutoff_weeks)\n            \n            if (np.sum(result_sub[\"mailing_uuid\"]) >= own_history_letters):\n                result_sub[\"state\"] = agg_lines[\"state\"][s]\n                result_sub[\"benefit\"] = agg_lines[\"benefit\"][s]\n                result_sub[\"attempt\"] = agg_lines[\"attempt\"][s]\n                result_all = result_all.append(result_sub, ignore_index = True)\n                \n        result_all[\"updated_at\"] = datetime.now()\n        result_all = result_all.drop(columns = [\"mailing_uuid\"])\n        \n        aggtable = bigquery_client.dataset(\"lab_forecast\").table(\"fcst_default_delay_distribution\")\n        \n        # insert in two steps\n        rows_to_insert = result_all[result_all['state'].isna()].dropna(axis=1).to_dict(orient='records')       \n        errors = bigquery_client.insert_rows(bigquery_client.get_table(aggtable), rows_to_insert)\n        if errors != []:\n            print(errors)\n\n        rows_to_insert = result_all.dropna().to_dict(orient='records')       \n        errors = bigquery_client.insert_rows(bigquery_client.get_table(aggtable), rows_to_insert)\n        if errors != []:\n            print(errors)\n\n        \n    else:\n        result_all = bigquery_client.query(\"\"\"SELECT * FROM lab_forecast.fcst_default_delay_distribution\n                                               WHERE updated_at = \n                                               (select max(updated_at) from lab_forecast.fcst_default_delay_distribution)\"\"\").to_dataframe()\n    \n    linetable = bigquery_client.dataset(\"lab_forecast\").table(\"fcst_line_delay_distribution\")\n    result_all_lines = pd.DataFrame()\n    \n    for s in range(len(lines_needed.index)):\n        line_id = lines_needed[\"line_uuid\"][s]\n        weight = lines_needed[\"delay_weight\"][s]\n        delay_df_line = delay_df.loc[(delay_df[\"line_id\"]==line_id),]\n        #print(delay_df_line)\n        result_line = pd.DataFrame()\n        if (delay_df_line.empty == False and weight > 0):\n            result_line = get_delay_frequency_table(delay_df_line, cutoff_weeks)\n            #print(result_line)\n            #print(line_id,\" has \",np.sum(result_line[\"mailing_uuid\"]),\" calls.\")\n        #else:\n        #    print(line_id,\" has no calls.\")\n        if (result_line.empty or weight < 1):\n            #print(lines_needed[\"line_uuid\"][s],\" needs help.\")\n            myprof = find_matching_profile(lines_needed[\"state\"][s], lines_needed[\"benefit\"][s], lines_needed[\"attempt\"][s], result_all)\n            #print(myprof)\n            if (weight > 0):\n                mergeprof = pd.merge(myprof,result_line, how = 'outer',on = 'period', suffixes=('_x', '_y'))\n                mergeprof[\"raw_value\"] = (1-weight)*mergeprof[\"raw_value_x\"].fillna(0) + weight*mergeprof[\"raw_value_y\"].fillna(0)\n                mergeprof[\"adj_value\"] = (1-weight)*mergeprof[\"adj_value_x\"].fillna(0) + weight*mergeprof[\"adj_value_y\"].fillna(0)\n                #print(mergeprof)\n                result_line = mergeprof\n            else:\n                result_line = myprof \n \n        result_line = result_line.loc[:, ['period', 'raw_value', 'adj_value']]    \n        result_line[\"line_uuid\"] = line_id\n        \n        if (result_all_lines.empty):\n            result_all_lines = result_line\n        else:\n            result_all_lines = result_all_lines.append(result_line, ignore_index = True)\n    result_all_lines[\"fcst_uuid\"] = fcst_id\n    rows_to_insert = result_all_lines.to_dict(orient='records')       \n    errors = bigquery_client.insert_rows(bigquery_client.get_table(linetable), rows_to_insert)\n    if errors != []:\n        print(errors)  \n    \n    return result_all_lines, result_all\n\n# function find_matching_profile: find the closest match for a forecast line with insufficient individual delay data\n\ndef find_matching_profile(state, benefit, attempt, result_all):\n    myprof = result_all.loc[(result_all[\"state\"]==state) & (result_all[\"benefit\"]==benefit) & (result_all[\"attempt\"]==attempt),]\n    if (myprof.empty):\n        #print('Using overall profile')\n        myprof = result_all.loc[pd.isnull(result_all[\"state\"]),]\n    #else:\n    #    print('Using state-benefit-attempt profile')\n    return myprof\n\n# function get_delay_frequency_table: performs aggregation and deals with any delays longer than cutoff_weeks\n\ndef get_delay_frequency_table(delay_df, cutoff_weeks):    \n    byweek=delay_df.groupby(\"period\", as_index=False)[\"mailing_uuid\"].count()\n    byweek[\"raw_value\"] = byweek[\"mailing_uuid\"]/np.sum(byweek[\"mailing_uuid\"])\n    byweek[\"adj_value\"] = byweek[\"raw_value\"]\n    \n    if byweek.loc[byweek[\"period\"]==cutoff_weeks+1].empty == False:\n        byweek.loc[byweek[\"period\"]==cutoff_weeks+1,\"adj_value\"] = 0\n        multiplier = 1/(1 - byweek.loc[byweek[\"period\"]==cutoff_weeks+1,\"raw_value\"].iloc[0])  \n        byweek[\"adj_value\"] = byweek[\"adj_value\"]*multiplier\n    return byweek\n",
      "metadata": {},
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": "# function plot_delay_profiles: plot the computed adjusted delay distributions, individual and pooled\n\ndef plot_delay_profiles(dflines, dfagg):\n    itable = pd.pivot_table(dflines, values='adj_value', index=['period'],\n                            columns=['line_uuid'], aggfunc=np.sum)\n    itable.plot()\n    dfagg[\"id\"] = dfagg[\"state\"] + \" + \" + dfagg[\"benefit\"] + \" + \" + dfagg[\"attempt\"].apply(str)\n    itable = pd.pivot_table(dfagg, values='adj_value', index=['period'],\n                            columns=['id'], aggfunc=np.sum)\n    itable.plot()",
      "metadata": {},
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": "# function compute_hist_rates: calls a SQL procedure that computes the adjusted historical response rates\n\ndef compute_hist_rates(fcst_id, params):\n    create_query = bigquery_client.query(\"\"\"CALL lab_forecast.compute_hist_rates('\"\"\" + fcst_id + \"\"\"',\"\"\" + str(round(params[\"response_weeks\"])) + \"\"\",\n                                    \"\"\" + str(round(params[\"forecast_horizon\"])) + \"\"\",\n                                    \"\"\" + str(round(params[\"offset_weeks\"])) + \"\"\")\"\"\")\n    create_query.result()",
      "metadata": {},
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": "# these functions perform different smoothing operations on the historical response rates\n# to come up with an initial \"future rate\" and corresponding confidence interval\n# forecast parameters determine which smoothing function will be used\n\n# obtain a response rate by performing exponential smoothing on historical weekly response rates\ndef get_smoothed_rate(resp_ts, alpha):\n    smooth_rate = resp_ts[\"rate\"][0]\n    for s in range(len(resp_ts.index)-1):\n        smooth_rate = resp_ts[\"rate\"][s+1]*(alpha) + smooth_rate*(1.0- alpha)\n    return smooth_rate\n\ndef get_smoothed_rate2(resp_ts, offset_weeks, forecast_horizon, drate, a):\n    model = ETSModel(endog=resp_ts[\"rate\"], error=\"add\", initialization_method=\"known\", initial_level=resp_ts[\"rate\"][0], \n                     trend=None, damped_trend=False, seasonal=None, seasonal_periods=1)\n    mfit = model.fit(smoothing_level=a, optimized=False)\n    point_forecast = mfit.forecast(offset_weeks+forecast_horizon-1)\n    ci = mfit.get_prediction(start = point_forecast.index[0],\n                                end = point_forecast.index[-1])\n    ci_lim = ci.pred_int(alpha = .05) #confidence interval\n    preds = pd.concat([point_forecast, ci_lim], axis = 1).reset_index(drop=True)\n    preds.columns = ['yhat', 'yhat_lower', 'yhat_upper']\n    #print(preds)\n    return preds\n\n# obtain a response rate by averaging historical weekly response rates\ndef get_average_rate(resp_ts):\n    return sum(resp_ts[\"rate\"]*resp_ts[\"letters\"])/sum(resp_ts[\"letters\"])\n\n# obtain a reponse rate series and CI by using the Holt-Winters method\ndef get_rate_with_trend_and_ci(resp_ts, offset_weeks, forecast_horizon, drate, tr = None, a = 0):\n    err = \"add\"\n    if (err == \"mul\"):\n        resp_ts[\"rate\"] = np.maximum(resp_ts[\"rate\"],0.001)\n        ivalue = 1\n    else:\n        ivalue = 0\n    if (len(resp_ts.index) < 10):\n        #print('initializing with ',drate)\n        model = ETSModel(endog=resp_ts[\"rate\"],  error = err, trend = tr,\n                         initialization_method=\"known\", initial_level=drate, initial_trend=ivalue, initial_seasonal=ivalue)\n    else:\n        model = ETSModel(endog=resp_ts[\"rate\"],  error = err, trend = tr)\n    if (tr == None):\n        with model.fix_params({'smoothing_level': a}):\n            mfit = model.fit()\n        #mfit = model.fit_constrained({'smoothing_level': a})\n    else:    \n        mfit = model.fit()\n    \n    point_forecast = mfit.forecast(offset_weeks+forecast_horizon-1)\n    ci = mfit.get_prediction(start = point_forecast.index[0],\n                                end = point_forecast.index[-1])\n    ci_lim = ci.pred_int(alpha = .05) #confidence interval\n    preds = pd.concat([point_forecast, ci_lim], axis = 1).reset_index(drop=True)\n    preds.columns = ['yhat', 'yhat_lower', 'yhat_upper']\n    #print(preds)\n    return preds",
      "metadata": {},
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": "# function compute_response_rates: uses one of the smoothing functions to come up with a response rate\n# pools it with an aggregated rate if there is insufficient history to generate a reliable rate\n\ndef compute_response_rates(fcst_id, params, lines_needed):\n    \n    rate_type = round(params[\"smoothing_type\"])\n    offset_weeks = round(params[\"offset_weeks\"])\n    forecast_horizon = round(params[\"forecast_horizon\"])\n    alpha = params[\"alpha\"]\n    \n    resp_ts =  bigquery_client.query(\"\"\"SELECT * FROM lab_forecast.fcst_hist_rates WHERE fcst_uuid = '\"\"\" + fcst_id + \"\"\"'\n                                        ORDER BY line_uuid, rate_date\"\"\").to_dataframe()\n    df = pd.DataFrame(columns=['fcst_uuid', 'line_uuid', 'rate','rate_lower_bound','rate_upper_bound','rate_trend'])\n    for s in range(len(lines_needed.index)):\n        line = lines_needed[\"line_uuid\"][s]\n        if (lines_needed[\"rate_weight\"][s] > 0):\n            resp_sub = resp_ts.loc[(resp_ts[\"line_uuid\"]==line),[\"rate_date\",\"letters\",\"rate\"]]\n            resp_sub = resp_sub.dropna().reset_index(drop=True)\n            #print(resp_sub)\n            if (rate_type in [0,1]):\n                if rate_type == 0:\n                    rate = get_average_rate(resp_sub)\n                else:\n                    rate = get_smoothed_rate(resp_sub, alpha)   \n                rate = lines_needed[\"rate_weight\"][s]*rate + (1.0 - lines_needed[\"rate_weight\"][s])*lines_needed[\"default_rate\"][s]    \n                result = [fcst_id, line, rate, rate, rate, 0]\n            else:\n                if rate_type == 2:\n                    preds = get_rate_with_trend_and_ci(resp_sub, offset_weeks, forecast_horizon, resp_sub[\"rate\"][0],None,alpha)\n                else:    \n                    preds = get_rate_with_trend_and_ci(resp_sub, offset_weeks, forecast_horizon, lines_needed[\"default_rate\"][s],\"add\")\n                result = [fcst_id,line] + preds.loc[offset_weeks-1,].values.flatten().tolist() + [preds[\"yhat\"][offset_weeks-1] - preds[\"yhat\"][offset_weeks-2]]\n        else:\n             result = [fcst_id, line, lines_needed[\"default_rate\"][s], 0, 2*lines_needed[\"default_rate\"][s], 0]\n        df.loc[len(df.index)] = result    \n    #print(df)\n    linetable = bigquery_client.dataset(\"lab_forecast\").table(\"fcst_rates\")\n    rows_to_insert = df.to_dict(orient='records')       \n    errors = bigquery_client.insert_rows(bigquery_client.get_table(linetable), rows_to_insert)\n    if (errors):\n        print(errors)\n\n        \n    ",
      "metadata": {},
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": "# function generate_fcst_volumes: generates the predicted number of calls from each line for each week in the forecast horizon\n\ndef generate_fcst_volumes(fcst_id, rdate, lines_needed, params):\n    forecast_horizon = round(params[\"forecast_horizon\"])\n    weekdays = round(params[\"weekdays\"])\n    \n    query = \"\"\"SELECT f.fcst_uuid, h.line_uuid,f.day as week, \n    SUM(h.letters*d.adj_value*ifnull(h.rate, GREATEST(r.rate - r.rate_trend*DATE_DIFF(f.reference_date,h.rate_date, WEEK),0))) volume,\n    SUM(h.letters*d.adj_value*ifnull(h.rate, GREATEST(r.rate_lower_bound - r.rate_trend*DATE_DIFF(f.reference_date,h.rate_date, WEEK),0))) volume_lower_bound,\n    SUM(h.letters*d.adj_value*ifnull(h.rate, GREATEST(r.rate_upper_bound - r.rate_trend*DATE_DIFF(f.reference_date,h.rate_date, WEEK),0))) volume_upper_bound \n    FROM `bdt-data-sci-prod.lab_forecast.fcst_hist_rates` h, `bdt-data-sci-prod.lab_forecast.fcst_line_delay_distribution` d,\n    `bdt-data-sci-prod.lab_forecast.fcst_rates` r,\n    (SELECT fcst_uuid, reference_date,day\n     FROM (SELECT fcst_uuid, reference_date\n      FROM lab_forecast.fcst\n     ) t JOIN\n     UNNEST(GENERATE_DATE_ARRAY(t.reference_date, DATE_ADD(t.reference_date,INTERVAL \"\"\" + str(forecast_horizon-1) + \"\"\" WEEK), INTERVAL 1 WEEK)) day\n     WHERE t.fcst_uuid = '\"\"\" + fcst_id + \"\"\"') f\n    WHERE r.fcst_uuid = f.fcst_uuid and h.fcst_uuid = f.fcst_uuid and h.line_uuid = r.line_uuid \n    and d.fcst_uuid = f.fcst_uuid and d.line_uuid = h.line_uuid\n    and d.period = DATE_DIFF(f.day,h.rate_date, WEEK)\n    group by 1,2,3\"\"\"\n    \n    if (weekdays == 0):\n        \n        iquery = bigquery_client.query(\"\"\"INSERT INTO `bdt-data-sci-prod.lab_forecast.fcst_values` \n                                       (fcst_uuid, line_uuid, fcst_period, volume, volume_lower_bound, volume_upper_bound)\"\"\" + query)\n        iquery.result()\n        \n        volumes = bigquery_client.query(query + \" order by 1,2,3\").to_dataframe()\n        volumes[\"fcst_period\"] = volumes[\"week\"].astype(str)\n        return volumes\n   \n    else:\n        calls = get_actual_calls(fcst_id, -2,-1)\n        daysopen = get_days_open(fcst_id, rdate, -2, forecast_horizon)\n        volumes = bigquery_client.query(query + \" order by 1,2,3\").to_dataframe()\n        \n        volumes[\"week\"] = volumes[\"week\"].astype(str)\n        mergeframe = pd.merge(lines_needed.loc[:,\"line_uuid\"], \n                              daysopen, how = \"cross\")\n        mergeframe = pd.merge(mergeframe,calls, how = 'left',on = ['week','line_uuid'])\n        mergeframe = pd.merge(mergeframe,volumes, how = 'left',on = ['week','line_uuid'])\n        mergeframe = mergeframe.fillna(0)\n        mergeframe[\"volume_diff\"] = 0.0 \n        mergeframe[\"fcst_uuid\"] = fcst_id\n        mergeframe[\"fcst_period\"] = mergeframe[\"week\"]\n        leftover = 0.0\n        for s in range(len(mergeframe)):\n            weekno = mergeframe[\"weekno\"][s]\n            if (weekno == 0):\n                leftover = 0.0\n            if (weekno < 2):\n                if (mergeframe[\"daysopen\"][s] < 5):\n                    leftover = leftover + mergeframe[\"numcalls\"][s]*(5.0/mergeframe[\"daysopen\"][s]-1.0)\n                else:\n                    leftover = 0.0\n            else:\n                if (mergeframe[\"daysopen\"][s] == 5):\n                    if (leftover > 0):\n                        mergeframe[\"volume_diff\"][s] = leftover\n                        leftover = 0.0\n                else:\n                    diff = mergeframe[\"volume\"][s]*(5.0/mergeframe[\"daysopen\"][s]-1.0)\n                    mergeframe[\"volume_diff\"][s] = - diff\n                    leftover = leftover + diff\n            #print(leftover)  \n        mergeframe[\"volume\"] = mergeframe[\"volume\"] + mergeframe[\"volume_diff\"]\n        mergeframe[\"volume_lower_bound\"] = np.maximum(mergeframe[\"volume_lower_bound\"] + mergeframe[\"volume_diff\"],0)\n        mergeframe[\"volume_upper_bound\"] = mergeframe[\"volume_upper_bound\"] + mergeframe[\"volume_diff\"]\n        #display(mergeframe)\n        \n        mergeframe = mergeframe.loc[mergeframe[\"volume\"] != 0.0, ['fcst_uuid', 'line_uuid', 'fcst_period','volume','volume_lower_bound','volume_upper_bound']]\n        vtable = bigquery_client.dataset(\"lab_forecast\").table(\"fcst_values\")\n        rows_to_insert = mergeframe.to_dict(orient='records')       \n        errors = bigquery_client.insert_rows(bigquery_client.get_table(vtable), rows_to_insert)\n        if (errors):\n            print(errors)\n        return mergeframe    \n\n        ",
      "metadata": {},
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": "# function get_actual_calls: gets the actual number of calls received each week for each line\n# for the purposes of displaying historical values and validation of historical forecasts\n\ndef get_actual_calls(fcst_id, start_week, end_week):   \n    calls = bigquery_client.query(\"\"\"\n    SELECT TO_HEX(MD5(concat(m.state, '##', m.letter_number, '##', m.mail_outreach_group, '##',m.mail_attempt, '##',m.mail_target_benefit))) as line_uuid, \n           cast(date_sub(date_TRUNC(pc.created_at, WEEK),INTERVAL 2 DAY) as date) as week, count(*) as numcalls \n    FROM \n    lab_forecast.fcst f, \n    `bdt-data-sci-prod.model_benefit_applications.f_mailing` m,\n    `bdt-data-sci-prod.model_benefit_applications.f_phone_call` pc\n    WHERE pc.last_touch_mailing_uuid = m.mailing_uuid\n    and f.fcst_uuid = '\"\"\" + fcst_id + \"\"\"' \n    and cast(date_sub(date_TRUNC(pc.created_at, WEEK),INTERVAL 2 DAY) as date) >= DATE_ADD(f.reference_date,INTERVAL \"\"\" + str(start_week) + \"\"\" WEEK)     \n    and cast(date_sub(date_TRUNC(pc.created_at, WEEK),INTERVAL 2 DAY) as date) <= DATE_ADD(f.reference_date,INTERVAL \"\"\" + str(end_week) + \"\"\" WEEK)     \n    GROUP BY 1,2\n    ORDER BY 1,2\"\"\").to_dataframe()\n    calls[\"week\"] = calls[\"week\"].astype(str)\n    return calls\n    ",
      "metadata": {},
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": "# function generate_weeks: generates a frame containing the list of weeks for the specified horizon (historical as well as forecast horizon)\n# for each week, the number of days the call center was/will be open is computed (this information is used to adjust response rates)\n\ndef generate_weeks(sdate,weeks):\n    startdate = np.datetime64(sdate)+np.timedelta64(2, \"D\")\n    enddate = startdate+np.timedelta64(7*weeks, \"D\")\n    dates = np.arange(np.datetime64(startdate), enddate, np.timedelta64(7, \"D\"))\n\n    cal = USFederalHolidayCalendar()\n    holidays = cal.holidays(start=startdate, end=enddate+np.timedelta64(7, \"D\")).to_pydatetime()\n    holidays = np.append(holidays,np.datetime64('2022-11-25'))\n    holidays = np.append(holidays,np.datetime64('2023-06-19'))   \n    #print(holidays)\n    \n    df = pd.DataFrame(dates, columns = ['week'])\n    df[\"daysopen\"] = 5\n    #df[\"weekyr\"] = np.array([xi.strftime(\"%V\") for xi in df[\"week\"]])\n    \n    for h in holidays:\n        for i in range(len(df.index)):\n            if (df[\"week\"][i]<=h and df[\"week\"][i]+np.timedelta64(7, \"D\") > h):\n                df[\"daysopen\"][i] = df[\"daysopen\"][i]-1\n    df[\"week\"] = df[\"week\"] - np.timedelta64(2, \"D\")\n    df[\"week\"] = df[\"week\"].astype(str)\n    df[\"daysopen\"] = df[\"daysopen\"].astype(float)\n    df[\"weekno\"] = df.index\n    return df",
      "metadata": {},
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": "def get_days_open(fcst_id, rdate, start_week, end_week):   \n    sdate = np.datetime64(rdate)+np.timedelta64(7*start_week, \"D\")\n    weeks = end_week - start_week\n    return generate_weeks(sdate,weeks)",
      "metadata": {},
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": "# compute the symmetric mean absolute error (SMAPE) between two vectors\ndef smape(A, F):\n    tmp = 2 * np.abs(F - A) / (np.abs(A) + np.abs(F))\n    tmp.fillna(0)\n    #len_ = np.count_nonzero(~np.isnan(tmp))\n    #if len_ == 0 and np.nansum(tmp) == 0: # Deals with a special case\n    #    return 1\n    return (1 / len(A)) * np.sum(tmp)\n\n# compute  vector of error mesures including MSE, SMAPE, difference in sums, percentage difference in sums\ndef compute_measures(actual, predicted):\n    #pr = np.array(predicted_raw)\n    #ac = np.array(actual_raw)\n    #goodi = np.logical_and(~np.isnan(pr), ~np.isnan(ac))\n    #predicted = pr[goodi]\n    #actual = ac[goodi]\n    if (len(actual)>0):\n        return [mean_squared_error(actual,predicted),\n                         smape(actual,predicted),\n                         np.sum(predicted) - np.sum(actual),\n                         (np.sum(predicted) - np.sum(actual))/np.sum(actual)] \n    else:\n        return [np.nan,np.nan,np.nan,np.nan]",
      "metadata": {},
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": "# function seq_visualize: generate a graph visualizing a sequential validation scheme\n\ndef seq_visualize(start_slice, stepsize_weeks, response_weeks, horizon_weeks, slices):\n    delta = timedelta(days=7*stepsize_weeks)\n    u = datetime.strptime(start_slice,\"%Y-%m-%d\")\n    dates = []\n    dates_s = []\n    dates_e = []\n    horizons = []\n    for i in range(slices):\n        dates.append(u)\n        dates_s.append(u-timedelta(days=7*response_weeks))\n        dend = min(u+timedelta(days=7*horizon_weeks), datetime.combine(datetime.now(), time.min) - timedelta(days=4) + relativedelta(weekday=FR(-1)))\n        dates_e.append(dend)\n        horizons.append(int((dend - u).days/7))\n        u = u + delta\n    \n    # Fixing random state for reproducibility\n    plt.rcdefaults()\n    plt.rcParams[\"figure.figsize\"] = (12,5)\n    fig, ax = plt.subplots()\n    y_pos = np.arange(slices)+1\n    ax.barh(y_pos, dates_e, color = 'r')\n    ax.barh(y_pos, dates, align='center')\n    ax.barh(y_pos, dates_s, align='center', color = 'w')\n    ax.invert_yaxis()  # labels read top-to-bottom\n    ax.set_title('Sequential Validation Scheme: '+str(slices)+' '+str(horizon_weeks)+'-week slices starting from '+start_slice+' using '+str(response_weeks)+' weeks of data')\n    ax.xaxis_date()\n    ax.set_xlim(dates[0]-timedelta(days=7*(response_weeks+1)),datetime.now())\n    plt.show()\n    return [date_obj.strftime('%Y-%m-%d') for date_obj in dates], horizons\n",
      "metadata": {},
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": "# function seq_validation: perform parameter tuning and sequential validation\n# this function cycles through all possible combinations of parameters within the supplied parameter list\n# generating a set of historical forecasts for each parameter set according to the specified validation scheme\n# and computing error measures that are displayed to the user in a convenient table\n\ndef seq_validation(start_slice, stepsize_weeks, slices, paramlist, r=26, horizon_weeks=1):\n    pdict_list = make_param_dict_list(paramlist, horizon_weeks)\n    statscols = ['MSE','SMAPE','SUM','PSUM']\n    statscombine = pd.DataFrame(columns = ['Params'] + statscols)\n    ds, hs = seq_visualize(start_slice, stepsize_weeks, r, horizon_weeks, slices)\n    j = 0\n    for ps in pdict_list:\n        j = j + 1\n        stats = pd.DataFrame(columns = ['Step'] + statscols)\n        for i in range(slices):\n            fcst_id, rdate, params, lines, df = fcst_main(0,0,'R',ds[i], ps)\n            if (hs[i]>0):\n                calls = get_actual_calls(fcst_id, 0,hs[i]-1)\n                daysopen = get_days_open(fcst_id, rdate, 0, hs[i])\n                mergeframe = pd.merge(lines.loc[:,\"line_uuid\"], daysopen, how = \"cross\")\n                mergeframe = pd.merge(mergeframe,calls, how = 'left',on = ['week','line_uuid'])\n                mergeframe = pd.merge(mergeframe,df, how = 'left',left_on = ['week','line_uuid'],right_on = ['fcst_period','line_uuid'])\n                mergeframe = mergeframe.fillna(0)\n                mergeframe.to_csv(\"accuracy\"+str(j)+\".csv\")\n                err = compute_measures(mergeframe[\"numcalls\"],mergeframe[\"volume\"])\n                stats.loc[len(stats.index)] = [ds[i]]+err                          \n        display(stats)\n        stats[\"SUM\"] = stats[\"SUM\"].abs()\n        stats[\"PSUM\"] = stats[\"PSUM\"].abs()\n        statscombine.loc[len(statscombine.index)] = [str(ps)] + list(stats.mean(axis = 0)) \n    #display(statscombine)\n    return statscombine\n\n# takes a dictionary containing lists of parameter values to try\n# returns a list of dictionaries containing every possible combination of parameters\n\ndef make_param_dict_list(paramlist_dict, horizon_weeks):\n    empty_dict = {\"forecast_horizon\":horizon_weeks}\n    new_dictlist = [empty_dict]\n    prev_dictlist = [empty_dict]\n    for key,valuelist in paramlist_dict.items():\n        #print(key, valuelist)\n        new_dictlist = []\n        for d in prev_dictlist:\n            for v in valuelist:\n                newd = d.copy()\n                newd[key] = v\n                new_dictlist.append(newd)\n        prev_dictlist = new_dictlist.copy()\n        #print(prev_dictlist)\n    return new_dictlist    \n\n",
      "metadata": {},
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": "# function fcst_main: generate the forecast \n# Args:\n#   live (int, optional): Flag to indicate if the forecast will be live. Default is 0 (not live).\n#   visible (int, optional): Flag to indicate if the (historic) forecast should be visible in the dashboard. Default is 0 (not visible).\n#   ftype (str, optional): Type of forecast: is this a response forecast (R) or a submission forecast (S). Default is 'R' (e.g., 'R' for regular).\n#   refdate (str, optional): Reference date for the forecast: i.e. let's pretend today is this date and we don't know anything that has happened since. \n#     Used for generating historical forecasts for validation. Default is an empty string, means today's date will be used.\n#   params (dict, optional): parameters for the forecast. Include only those parameters where you want to overrride the default parameter value.\n#                Default is an empty dictionary.\n#\n# Returns:\n#   fcst_id (uuid): unique identifier for the forecast\n#   rdate (date): reference date adjusted to previous Friday\n#   params (list): values for all parameters (default or overridden)\n#   lines (dataframe): list of forecast lines with their properties\n#   df (dataframe): actual and predicted call volume time series \n\ndef fcst_main(live = 0, visible = 0, ftype = 'R', refdate = '',ps = {}):\n    fcst_id, rdate, params = initialize_forecast(live, visible, ftype, refdate, ps)\n    lines = create_lines(fcst_id, params)\n    dflines,dfagg = get_delay_frequencies(fcst_id, params, lines)\n    compute_hist_rates(fcst_id, params)\n    compute_response_rates(fcst_id, params, lines)\n    df = generate_fcst_volumes(fcst_id, rdate, lines, params)\n    return fcst_id, rdate, params, lines, df",
      "metadata": {},
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "source": "# THIS IS THE \"MAIN\" FUNCTION #\n# THIS IS WHAT NEEDS TO RUN EVERY TUESDAY #\n\nfcst_id, rdate, params, lines, df = fcst_main(0,0) # FOR TESTING, USE (0,0) IN PRODUCTION, USE (1,1)\n\n# THIS IS THE \"MAIN\" FUNCTION #",
      "metadata": {},
      "outputs": [],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}